<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<script async="" src="js/analytics.js"></script>
	</script>
	<script src="js/jquery-1.js"></script>
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="css/bootstrap-theme.css">
	<script src="js/bootstrap.js"></script>
	<style type="text/css">
		/* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
		a {
			color: #1772d0;
			text-decoration: none;
		}

		a:focus,
		a:hover {
			color: #f09228;
			text-decoration: none;
		}

		body,
		td,
		th {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 16px;
			font-weight: 400
		}

		heading {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 19px;
			font-weight: 1000
		}

		strong {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 16px;
			font-weight: 800
		}

		strongred {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			color: 'red';
			font-size: 16px
		}

		sectionheading {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 22px;
			font-weight: 600
		}
	</style>
	<!-- <link rel="icon" type="image/png" href="images/cvit.jpg"> -->
	<link rel="icon" type="image/png" href="images/uoe.png">
	<script type="text/javascript" src="js/common.js"></script>
	<title>Anil Batra</title>
	<meta name="Anil Batra's Personal Homepage" http-equiv="Content-Type" content="Anil Batra's Personal Homepage">
	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
		rel='stylesheet' type='text/css'>
	<!-- Start : Google Analytics Code -->
	<!-- <script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-64069893-1', 'auto');
	ga('send', 'pageview');
</script> -->
	<!-- End : Google Analytics Code -->
	<!-- Scramble Script by Jeff Donahue -->
	<!-- <script src="js/scramble.js"></script> -->
</head>

<body>
	<table width="100%" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr>
			<td>
				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<p align="center">
						<font size="7">Anil Batra</font><br>
						<!-- <b>Email</b>: <i></i> -->
						<!-- <font id="email" style="display:inline;">
						<noscript><i>Please enable Javascript to view</i></noscript>
					</font> -->
						<!-- <script>
						emailScramble = new scrambledString(document.getElementById('email'),
							'emailScramble', 'erepc@aahby.etulk.skde',
							[12,13,15,1,8,7,2,5,4,11,18,10,17,3,22,16,6,19,9,14,21,20]);
						</script> -->
					</p>
					<tr>
						<td width="70%" valign="middle" align="justify">
							<p>I am a <a href="https://web.inf.ed.ac.uk/cdt/natural-language-processing"
								target="_blank">CDT</a> Ph.D. scholar at <a
								href="https://www.ed.ac.uk/informatics target=" _blank">School of Informatics</a>
								in University of Edinburgh and being supervised by <a
								href="http://homepages.inf.ed.ac.uk/keller/" target="_blank">Prof. Frank
								Keller</a> and <a href="https://laurasevilla.me/" target="_blank">Dr. Laura Sevillia</a>. 
								My research interests lie at the intersection of <i> Language </i> and <i>Vision</i>.
								Specifically I enjoy working on:
								<ul>
									<li>Long and Untrimmed Video and Text Understanding,</li>
									<li>Undupervised and Self-Supervised representation learning for multimodal data under low resources,</li>
									<li>Understanding temporal dynamics and reasoning</li>
								</ul>
								
							<p>I also enjoy reading works related to Geospatial data, large language models and how to make models more reliable.
								Previously, I have completed my Master in Computer Science (by research) at <a
									href="https://www.iiit.ac.in/" target="_blank">IIIT - Hyderabad</a>,
								under the supervision of <a href="https://faculty.iiit.ac.in/~jawahar/"
									target="_blank">Prof. C.V. Jawahar</a> and Facebook mentors
								<a href="https://research.fb.com/people/pang-guan/" target="_blank">Dr. Guan Pang</a>,
								<a href="https://csc.lsu.edu/~saikat/" target="_blank">Dr. Saikat Basu</a>.
								During Masters, I was part of <a href="https://cvit.iiit.ac.in/">Center of Visual
									Information Technology Lab (CVIT)</a> and developed models to detect roads under occlusion in Satellite Imagery.</p>
								<!-- I am a part of <a href="https://cvit.iiit.ac.in/" >Center of Visual Information Technology Lab (CVIT)</a>, advised by <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank">Prof. C.V. Jawahar</a>. My research interests are <i>Computer Vision</i> and <i>Deep Learning</i>.  -->
								<!-- I have submitted my <a href="papers/MSThesis-compressed.pdf">thesis</a> in December 2018 on <i>Road Topology Extraction from Satellite images by Knowledge Sharing</i>.</p> -->

							<p>I worked as <strong>Research Engineer</strong> at <i>Facebook</i> in Spatial Computing
								Team. I was designing, training, and evaluating extraction of connected road network
								with limited set of labels and large scale noisy labels. </p>
							<!-- <p>Earlier, I did my Bachelor's in Electronics and Communications from <a
									href="https://www.rimt.ac.in/" target="_blank">RIMT</a> affiliated to <a
									href="https://ptu.ac.in/" target="_blank">Punjab Technical University</a>,
								Jalandhar.</p> -->

							<p align=center>
								<a href="mailto:anilbatra2185@gmail.com" style="padding:5 20 5 25"><img
										src="images/email.png" alt="anilbatra2185@gmail.com"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Email</span></a> | 
								<a href="AnilBatra_CV.pdf" target="_blank" style="padding:5 20 5 25"><img
										src="images/cv.png" style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">CV</span></a> | 
								<a href="https://scholar.google.co.in/citations?hl=en&user=C9rsD2UAAAAJ" target="_blank"
									style="padding:5 20 5 25"><img src="images/scholar.jpg"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Google Scholar</span></a> | 
								<a href="https://github.com/anilbatra2185" target="_blank" style="padding:5 20 5 25"><img
										src="images/github.png" style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Github</span></a> | 
								<a href="https://www.linkedin.com/in/anil-batra/" target="_blank"
									style="padding:5 5 5 25"><img src="images/linkedin.png"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">LinkedIn</span></a>
								<!-- <a href="https://twitter.com/_anilbatra?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false" style="padding:5 10 5 25">
									Follow @_anilbatra</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->
							</p>
						</td>
						<td width="20%" valign="top">
							<table>
								<tr>
									<td><a href="images/anilbatra.jpg"><img src="images/anilbatra.jpg" width="100%"></a></td>
								</tr>
								<tr>
									<td style="text-align: center;"><a href="https://twitter.com/_anilbatra?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @_anilbatra</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></td>
								</tr>
							</table>
						</td>
					</tr>
				</table>

				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td>
							<sectionheading>News</sectionheading>
							<ul>
								<li> <b>[Sep 2022]</b>: Our new work "Temporal Ordering in the Segmentation of
									Instructional Videos" is accepted at
									<a href="https://bmvc2022.org/programme/papers/" target="_blank">BMVC 2022</a>.
								</li>
								<li> <b>[Dec 2021]</b>: Volunteer for session chair at <a
										href="https://neurips.cc/virtual/2021/affinity-workshop/22884"
										target="_blank">LXAI</a> workshop,
									<a href="https://neurips.cc/" target="_blank">Neurips 2021</a>.
								</li>
								<li> <b>[Nov 2021]</b>: Will be serving as CVPR 2022 Reviewer.</li>
								<li> <b>[Jun 2021]</b>: Served as ICCV 2021 Reviewer.</li>
								<li> <b>[Sep 2020]</b>: Joined <a
										href="https://web.inf.ed.ac.uk/cdt/natural-language-processing"
										target="_blank">CDT-NLP</a> Ph.D at University of Edinburgh under the
									supervision of <a href="https://laurasevilla.me/" target="_blank">Dr. Laura
										Sevillia</a> and <a href="http://homepages.inf.ed.ac.uk/keller/"
										target="_blank">Prof. Frank Keller</a>.</li>
								<li> <b>[Jun 2019]</b>: Succesfully defended my Master thesis. Panel - <a
										href="https://faculty.iiit.ac.in/~jawahar/" target="_blank">Prof. C.V.
										Jawahar</a>, <a href="https://www.iiit.ac.in/people/faculty/mkrishna/"
										target="_blank">Prof. K. Madhava Krishna</a>, <a href="https://girishvarma.in/"
										target="_blank">Dr. Girish Varma</a></li>
								<li> <b>[Jun 2019]</b>: Poster presentation at CVPR 2019, Long Beach (<a
										href="images/cvpr_poster_present.jpg" target="_blank">image</a>).</li>
								<li> <b>[May 2019]</b>: Received travel sponsporship from Facebook - Spatial Computing
									team to attend CVPR 2019.</li>
								<li> <b>[Apr 2019]</b>: Presented <i>CVPR - Improving Road Connectivity</i> work at
									Facebook Spatial Team.</li>
								<li> <b>[Mar 2019]</b>: Paper accepted at CVPR 2019 on Improved Road Connectivity.</li>
								<li> <b>[Jan 2019]</b>: Join <i>Facebook - Spatial Team</i> as Research Engineer
									(Contingent Worker).</li>
								<li> <b>[Dec 2018]</b>: Submitted my Master <a
										href="papers/MSThesis-compressed.pdf">thesis</a> <i>Road Topology Extraction
										from Satellite images by Knowledge Sharing</i>.</li>
								<li> <b>[Jun 2018]</b>: Paper accepted at BMVC 2018 on <i>Self-Supervised Learning</i>.
								</li>
							</ul>
						</td>
					</tr>
				</table>

				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td>
							<sectionheading>Publications</sectionheading>
						</td>
					</tr>
				</table>

				<table width="70%" align="center">
					<!-- <tr>
						<td width="25%" valign="top" align="center"><a href="images/noimage.png">
								<img src="images/noimage.png" alt="sym" width="100%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="#" id="ai2024">
									<img src="images/new.png" alt="[NEW]" width="50px" height="30px" style="border-style: none">
									<heading>LowRep: Low Resource Pre-Training for Dense Video Captioning</heading>
								</a><br>
								<strong>Anil Batra</strong>, Davide Moltisanti, Marcus Rohrbach, Laura Sevilla-Lara, Frank Keller<br>
								<em>Under Review</em> 2023 <br>
							</p>
							<div class="paper" id="ai2024">
								<a href="#" target="_blank" class="buttonTT">pdf</a>
								<a href="#" target="_blank" class="buttonTT">suppl</a>
								<a data-toggle="collapse" data-parent="#ai2024" href="#ai2024_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#ai2024" data-toggle="collapse" href="#ai2024-list"
									class="buttonSS">bibtex</a>
								<br> 

								<div id="ai2024_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Dense Video Captioning (DVC) is an important task in video
										understanding as it empowers AI systems to grasp procedural
										knowledge. Crafting annotations for DVC comes with substantial 
										expenses, given that it mandates identifying the time span of each 
										instruction while generating a textual captions of the procedure in the video. 
										Consequently, the datasets for this task tend to be of limited size, making it challenging
										for modern AI systems to effectively learn from such data.
										The success of large scale pre-training has motivated recent
										works to pursue the pre-training of big models using large
										and noisy video-transcript datasets to enhance performance.
										Nonetheless, the sheer magnitude of these datasets (≈15M videos) demands 
										costly computational resources. In this work we propose an alternative approach
										and develop a technique to automatically curate a smaller and less noisy dataset: IVI-RS 
										(Instructional Videos with Recipe Steps). Our IVI-RS dataset is one order of 
										magnitude smaller than the present web-scaled dataset, yet it remains fully capable of
										efficiently training large-scale models. In order to fully harness the potential of 
										IVI-RS, we propose a larger transformer model designed for end-to-end captioning. When our model
										is pre-trained on IVI-RS, it achieves state-of-the-art performance across various
										metrics on YouCook2 and TASTY, while demanding only a fraction of computational resources.
										We boost localization performance (SODA-D) by 16% and captioning performance (SODA-C) by 19% on both datasets
										</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="ai2024-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@InProceedings{lowrep_batra,<br>
										author = {Batra, Anil and Moltisanti, Davide and Rohrbach, Marcus and Sevilla-Lara, Laura and Keller, Frank},<br>
										title = {LowRep: Low Resource Pre-Training for Dense Video Captioning},<br>
										booktitle = {UnderReview},<br>
										year = {2023} <br>
										}

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr> -->
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/BMVC2022_thumbnail.png">
								<img src="images/BMVC2022_thumbnail.png" alt="sym" width="100%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="http://arxiv.org/abs/2209.15501" id="bmvc22">
									<!-- <img src="images/new.png" alt="[NEW]" width="50px" height="30px" style="border-style: none"> -->
									<heading>A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos</heading>
								</a><br>
								<strong>Anil Batra</strong>, Shreyank N Gowda, Frank Keller, Laura Sevilla-Lara<br>
								<em>British Machine Vision Conference</em> (BMVC), 2022 <br>
							</p>
							<div class="paper" id="bmvc22">
								<a href="https://arxiv.org/pdf/2209.15501.pdf" target="_blank" class="buttonTT">pdf</a>
								<a href="papers/BMVC2022_SODA-D_Suppl.pdf" target="_blank" class="buttonTT">suppl</a>
								<!-- <a href="papers/cvpr2019_poster.pdf" target="_blank" class="buttonTT">poster</a> -->
								<a data-toggle="collapse" data-parent="#bmvc22" href="#bmvc22_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#bmvc22" data-toggle="collapse" href="#bmvc22-list"
									class="buttonSS">bibtex</a>
								<!-- <a href="https://github.com/anilbatra2185/road_connectivity" target="_blank"
									class="buttonTT">code</a> -->
								<br>

								<div id="bmvc22_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Understanding the steps required to perform a task is an important skill for AI systems. 
										Learning these steps from instructional videos involves two subproblems: (i) identifying 
										the temporal boundary of sequentially occurring segments and (ii) summarizing
										these steps in natural language. We refer to this task as Procedure Segmentation and
										Summarization (PSS). In this paper, we take a closer look at PSS and propose three
										fundamental improvements over current methods. The segmentation task is critical, as
										generating a correct summary requires each step of the procedure to be correctly identified. 
										However, current segmentation metrics often overestimate the segmentation quality
										because they do not consider the temporal order of segments. In our first contribution,
										we propose a new segmentation metric that takes into account the order of segments,
										giving a more reliable measure of the accuracy of a given predicted segmentation. Current 
										PSS methods are typically trained by proposing segments, matching them with the
										ground truth and computing a loss. However, much like segmentation metrics, existing
										matching algorithms do not consider the temporal order of the mapping between candidate 
										segments and the ground truth. In our second contribution, we propose a matching
										algorithm that constrains the temporal order of segment mapping, and is also differentiable. 
										Lastly, we introduce multi-modal feature training for PSS, which further improves
										segmentation. We evaluate our approach on two instructional video datasets (YouCook2
										and Tasty) and observe an improvement over the state-of-the-art of ∼ 7% and ∼ 2.5%
										for procedure segmentation and summarization, respectively.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="bmvc22-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@InProceedings{batraBMVC2022pss,<br>
										author = {Batra, Anil and Gowda, Shreyank N and Keller, Frank and Sevilla-Lara, Laura},<br>
										title = {A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos},<br>
										booktitle = {BMVC},<br>
										year = {2022} <br>
										}

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/cvpr_poster_method.png">
								<img src="images/cvpr_poster_method-min.png" alt="sym" width="100%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="https://github.com/anilbatra2185/road_connectivity/" id="cvpr19">
									<!--<img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">-->
									<heading>Improved Road Connectivity by Joint Learning of Orientation and
										Segmentation</heading>
								</a><br>
								<strong>Anil Batra*</strong>, Suriya Singh*, Guan Pang, Saikat Basu, C.V. Jawahar and
								Manohar Paluri (* equal contribution)<br>
								<em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019 <br>
							</p>
							<div class="paper" id="cvpr19">
								<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Batra_Improved_Road_Connectivity_by_Joint_Learning_of_Orientation_and_Segmentation_CVPR_2019_paper.pdf"
									target="_blank" class="buttonTT">pdf</a>
								<a href="papers/RoadConnectivity_CVPR_Supplementary.pdf" target="_blank"
									class="buttonTT">suppl</a>
								<a href="papers/cvpr2019_poster.pdf" target="_blank" class="buttonTT">poster</a>
								<a data-toggle="collapse" data-parent="#cvpr19" href="#cvpr19_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#cvpr19" data-toggle="collapse" href="#cvpr19-list"
									class="buttonSS">bibtex</a>
								<a href="https://github.com/anilbatra2185/road_connectivity" target="_blank"
									class="buttonTT">code</a>
								<br>

								<div id="cvpr19_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Road network extraction from satellite images often produce fragmented road
										segments leading to road maps unfit for real applications. Pixel-wise
										classification fails to predict topologically correct and connected road masks
										due to the absence of connectivity supervision and difficulty in enforcing
										topological constraints. In this paper, we propose a connectivity task called
										Orientation Learning, motivated by the human behavior of annotating roads by
										tracing it at a specific orientation. We also develop a stacked multi-branch
										convolutional module to effectively utilize the mutual information between
										orientation learning and segmentation tasks. These contributions ensure that the
										model predicts topologically correct and connected road masks. We also propose
										Connectivity Refinement approach to further enhance the estimated road networks.
										The refinement model is pre-trained to connect and refine the corrupted
										ground-truth masks and later fine-tuned to enhance the predicted road masks. We
										demonstrate the advantages of our approach on two diverse road extraction
										datasets SpaceNet and DeepGlobe. Our approach improves over the state-of-the-art
										techniques by 9% and 7.5% in road topology metric on SpaceNet and DeepGlobe,
										respectively.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="cvpr19-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@InProceedings{Batra_2019_CVPR,<br>
										author = {Batra, Anil and Singh, Suriya and Pang, Guan and Basu, Saikat and
										Jawahar, C.V. and Paluri, Manohar},<br>
										title = {Improved Road Connectivity by Joint Learning of Orientation and
										Segmentation},<br>
										booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition
										(CVPR)},<br>
										month = {June},<br>
										year = {2019} <br>
										}

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" height="80%" valign="top" align="center"><a
								href="images/overview_bmvc_logo.png">
								<img src="images/overview_bmvc_logo.png" alt="sym" width="100%"
									style="border-style: none"></a>
						</td>
						<td width="75%" valign="top">
							<p>
								<a href="http://bmvc2018.org/contents/papers/0345.pdf" id="ICLR19">
									<heading>Self-supervised Feature Learning for Semantic Segmentation of Overhead
										Imagery</heading>
								</a><br>
								Suriya Singh*, <strong>Anil Batra*</strong>, Guan Pang, Lorenzo Torresani, Saikat Basu,
								C.V. Jawahar and Manohar Paluri (* equal contribution)<br>
								<em>British Machine Vision Confernece (BMVC)</em>, 2018<br>
							</p>

							<div class="paper" id="bmvc18">
								<a href="http://bmvc2018.org/contents/papers/0345.pdf" target="_blank"
									class="buttonTT">pdf</a>
								<a href="papers/Self_supervised_Feature_Learning_for_Semantic_Segmentation_of_Overhead_Imagery_Supplementary.pdf"
									target="_blank" class="buttonTT">suppl</a>
								<a data-toggle="collapse" data-parent="#bmvc18" href="#bmvc18_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#bmvc18" href="#bmvc18-list" class="buttonSS"
									data-toggle="collapse">bibtex</a>

								<div id="bmvc18_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Overhead imageries play a crucial role in many applications such as urban
										planning, crop yield forecasting, mapping, and policy making. Semantic
										segmentation could enable automatic, efficient, and large-scale understanding of
										overhead imageries for these applications. However, semantic segmentation of
										overhead imageries is a challenging task, primarily due to the large domain gap
										from existing research in ground imageries, unavailability of large-scale
										dataset with pixel-level annotations, and inherent complexity in the task.
										Readily available vast amount of unlabeled overhead imageries share more common
										structures and patterns compared to the ground imageries, therefore, its
										large-scale analysis could benefit from unsupervised feature learning
										techniques. <br /> In this work, we study various self-supervised feature
										learning techniques for semantic segmentation of overhead imageries. We choose
										image semantic inpainting as a self-supervised task for our experiments due to
										its proximity to the semantic segmentation task. We (i) show that existing
										approaches are inefficient for semantic segmentation, (ii) propose architectural
										changes towards self-supervised learning for semantic segmentation, (iii)
										propose an adversarial training scheme for self-supervised learning by
										increasing the pretext task's difficulty gradually and show that it leads to
										learning better features, and (iv) propose a unified approach for overhead scene
										parsing, road network extraction, and land cover estimation. Our approach
										improves over training from scratch by more than 10% and ImageNet pre-trained
										network by more than 5% mIoU.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>

								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="bmvc18-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@inproceedings{singhBMVC18overhead, <br>
										Author = {Singh, Suriya; Batra, Anil; Pang, Guan; Torresani, Lorenzo; Basu,
										Saikat; Paluri, Manohar; Jawahar, C. V.}, <br>
										Title = {Self-supervised Feature Learning for Semantic Segmentation of Overhead
										Imagery}, <br>
										Booktitle = {BMVC},
										Year = {2018} <br>
										}

									</p>
								</div>
							</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>

	</table>

	<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr>
			<td>
				<sectionheading>Projects</sectionheading>
			</td>
		</tr>
	</table>

	<table width="70%" align="center">
		<tr>
			<td width="25%" valign="top" align="center"><a href="images/ip_report_motivation.png">
					<img src="images/ip_report_motivation.png" alt="sym" width="100%" style="border-style: none"></a>
			</td>
			<td width="75%" valign="top">
				<p><a href="papers/Project_Technical_Report.pdf" id="cdt2020">
						<heading>Multimodal Procedural Knowledge Learning using WikiHow articles</heading>
					</a><br>
					<em>CDT-NLP Project (2020)</em><br>
				</p>
				<div class="paper" id="cdt20">
					<a href="papers/Project_Technical_Report.pdf" target="_blank" class="buttonTT">pdf</a>
					<a data-toggle="collapse" data-parent="#cdt20" href="#cdt20_abstract-list"
						class="buttonAA">abstract</a>
					<div id="cdt20_abstract-list" class="panel-collapse collapse out"
						style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
						<p style="margin:0px 0px 0px 0px;"></p>
						<p style="font-size:14.1px; text-align: justify;">
							Procedural learning with multi-modal <i>'how-to'</i> articles is beneficial to enable AI
							systems with an ability to perform goal oriented tasks. Learning the temporal event structure in
							procedures through only-text based datasets fails to capture the implicit information among events e.g.
							missing object of an action. We hypothesize that the visual data is adequate to augment the missing
							information and extend the text based dataset
							<a href="https://aclanthology.org/2020.emnlp-main.374/" target="_blank">(Zhang et.al.)</a>
							with visual data. Towards our goal, we study pairwise event ordering with architectures pre-trained on uni and multi modal data.
							Surprisingly, we find that joining the features from architectures (Resnet-50 + BERT) which are
							pre-trained on uni-modal data, is superior to state-of-the-art multi-modal architectures (LXMERT and UNITER) towards
							temporal structure learning. Furthermore, we enhance the event relation learning with an attention
							mechanism. Our experiments on the extended pairwise step-order dataset shows that our approach benefit
							in learning the perfect order by 1.67% in comparison to text-only datasets.
						</p>
						<p style="margin:0px 0px 0px 0px;"></p>
					</div>
				</div>
				</div>
			</td>
		</tr>
		<tr>
			<td width="25%" valign="top" align="center"><a href="images/motion_field.png">
					<img src="images/motion_field.png" alt="sym" width="100%" style="border-style: none"></a>
			</td>
			<td width="75%" valign="top">
				<p><a href="" id="cvpr19">
						<heading>Motion Field Estimation using MRF</heading>
					</a><br>
					<em>DIP Course Project</em><br>
				</p>
				<div class="paper" id="dip_project">
					<a href="papers/DIP_Project_motion_field.pdf" target="_blank" class="buttonTT">pdf</a>
					<a data-toggle="collapse" data-parent="#bmvc18" href="#dip_project-list"
						class="buttonAA">abstract</a>
					<div id="dip_project-list" class="panel-collapse collapse out"
						style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
						<p style="margin:0px 0px 0px 0px;"></p>
						<p style="font-size:14.1px; text-align: justify;">
							Study and analysis of edge flow in images to estimate the motion of different objects. In
							the course project, I use discrete Markov Random Field (MRF) to extract the motion of each
							object from synthetic and natural image sequences.
						</p>
						<p style="margin:0px 0px 0px 0px;"></p>
					</div>
				</div>
				</div>
			</td>
		</tr>
	</table>

	<table width="70%" align="center" border="0" cellpadding="20">
		<tr>
			<td>
				<sectionheading>Teaching</sectionheading>
			</td>
		</tr>
		<tr>
			<td width="10%"><img src="images/noimage.png" alt="Data Mining & Exploration" width="100%"></td>
			<td width="80%" valign="center">
				<p>
					<heading><a href="https://www.inf.ed.ac.uk/teaching/courses/dme/2021/details.html"
							target="_blank">INFR11007</a>: Data Mining and Exploration (Spring 2021)</heading><br>
					<strong>Instructors</strong>: <a href="https://michaelgutmann.github.io/">Dr. Michael Gutmann</a>,
					<a href="https://homepages.inf.ed.ac.uk/snaraya3/index.html">Dr. Siddharth N.</a><br>
				</p>
			</td>
		</tr>
		<tr>
			<td width="10%"><img src="images/dip.png" alt="Digital Image Processing" width="100%"></td>
			<td width="80%" valign="center">
				<p>
					<heading>CS478: Digital Image Processing (Monsoon 2018) </heading><br>
					<strong>Instructors</strong>: <a href="https://ravika.github.io">Dr. Ravi Kiran Sarvadevabhatla</a>,
					<a href="https://www.linkedin.com/in/rajvishah/">Rajvi Shah</a><br>
				</p>
			</td>
		</tr>
		<tr>
			<td width="10%"><img src="images/tsprint_aiml.png" alt="pacman" width="100%"></td>
			<td width="80%" valign="center">
				<p>
					<heading>Mentor in 1st foundations course on Artificial Intelligence and Machine Learning</heading>
					<br>
					<strong>Instructor</strong>: <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C.V. Jawahar</a>
					<br>
				</p>
			</td>
		</tr>
	</table>


	<table width="70%" align="center" border="0" cellpadding="20">
		<tr>
			<td>
				<sectionheading>Professional Activities</sectionheading>
				<ul>
					<li> Reviewer: ICCV 2021, 2023 | CVPR 2022, 2023 | ECCV 2022
						<!-- <a href="http://iccv2021.thecvf.com/home" target="_blank">ICCV 2021</a>,
						<a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>,
						<a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> -->
					</li>
				</ul>
			</td>
		</tr>
		<tr>
			<td>
				<sectionheading>Selected Awards</sectionheading>
				<ul>
					<li> Awarded funding for 4 years by <a href="https://www.ed.ac.uk/informatics"
							target="_blank">School of Informatics</a> and <a
							href="https://gtr.ukri.org/person/B48C147D-9AB7-4E89-8144-20FAD3E0048D"
							target="_blank">UKRI</a> for Ph.D.</li>
					<li> Facebook Travel Support to attend CVPR 2019</li>
					<li><a href="https://en.wikipedia.org/wiki/Graduate_Aptitude_Test_in_Engineering">GATE</a> - EC
						qualified with 254 rank in 2009.</li>
					<li> Gold Medal in Electronics and Communication at RIMT-Institute of Engineering and Technology,
						affiliated to PTU-Jalandhar (2007)</li>
				</ul>
			</td>
		</tr>
	</table>

	<table width="30%" align="right" border="0" cellspacing="0" cellpadding="20">
		<tr>
			<td>
				<p align="right">
					<font size="2">
						Template: <a href="https://people.eecs.berkeley.edu/~pathak/">this</a>, <a
							href="http://www.cs.berkeley.edu/~sgupta/">this</a> and <a
							href="http://jeffdonahue.com/">this</a>
					</font>
				</p>
			</td>
			<td>
				<p align="right">
					<font size="2"></font>
					<a href="https://hits.seeyoufarm.com"><img
							src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=http%3A%2F%2Fanilbatra2185.github.io&count_bg=%23A4C78A&title_bg=%23929191&icon=apollographql.svg&icon_color=%23C3E508&title=Visitors&edge_flat=false" /></a>
				</p>
			</td>
		</tr>
	</table>

	</td>
	</tr>
	</table>
	<script xml:space="preserve" language="JavaScript">
		hideallbibs();
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('bmvc22_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('cvpr19_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('bmvc18_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('cdt20_abstract');
	</script>
</body>

</html>