<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<script async="" src="js/analytics.js"></script>
	</script>
	<script src="js/jquery-1.js"></script>
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="css/bootstrap-theme.css">
	<script src="js/bootstrap.js"></script>
	<style type="text/css">
		/* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
		a {
			color: #1772d0;
			text-decoration: none;
		}

		a:focus,
		a:hover {
			color: #f09228;
			text-decoration: none;
		}

		body,
		td,
		th {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 16px;
			font-weight: 400
		}

		heading {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 19px;
			font-weight: 1000
		}

		strong {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 16px;
			font-weight: 800
		}

		strongred {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			color: 'red';
			font-size: 16px
		}

		sectionheading {
			font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
			font-size: 22px;
			font-weight: 600
		}
	</style>
	<!-- <link rel="icon" type="image/png" href="images/cvit.jpg"> -->
	<link rel="icon" type="image/png" href="images/uoe.png">
	<script type="text/javascript" src="js/common.js"></script>
	<title>Anil Batra</title>
	<meta name="Anil Batra's Personal Homepage" http-equiv="Content-Type" content="Anil Batra's Personal Homepage">
	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
		rel='stylesheet' type='text/css'>
	<!-- Start : Google Analytics Code -->
	<!-- <script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-64069893-1', 'auto');
	ga('send', 'pageview');
</script> -->
	<!-- End : Google Analytics Code -->
	<!-- Scramble Script by Jeff Donahue -->
	<!-- <script src="js/scramble.js"></script> -->
</head>

<body>
	<table width="100%" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr>
			<td>
				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<p align="center">
						<font size="7">Anil Batra</font><br>
						<!-- <b>Email</b>: <i></i> -->
						<!-- <font id="email" style="display:inline;">
						<noscript><i>Please enable Javascript to view</i></noscript>
					</font> -->
						<!-- <script>
						emailScramble = new scrambledString(document.getElementById('email'),
							'emailScramble', 'erepc@aahby.etulk.skde',
							[12,13,15,1,8,7,2,5,4,11,18,10,17,3,22,16,6,19,9,14,21,20]);
						</script> -->
					</p>
					<tr>
						<td width="70%" valign="middle" align="justify">
							<p>I am a <a href="https://web.inf.ed.ac.uk/cdt/natural-language-processing"
								target="_blank">CDT</a> Ph.D. scholar at <a
								href="https://www.ed.ac.uk/informatics target=" _blank">School of Informatics</a>
								in University of Edinburgh and working with <a
								href="http://homepages.inf.ed.ac.uk/keller/" target="_blank">Prof. Frank
								Keller</a>, <a href="https://rohrbach.vision/" target="_blank">Prof. Marcus Rohrbach</a>, 
								and <a href="https://laurasevilla.me/" target="_blank">Dr. Laura Sevillia</a>. 
								My research interests are centered at the intersection of Language and Vision, with a focus on developing models that can plan, 
								reason, and execute goal-oriented tasks involving multiple complex events through text comprehension and video analysis. 
								Currently, my work involves analyzing long procedural videos and text to understand and ground the temporal structure of events. 
								This research is directed towards developing efficient models that can accurately capture the sequence and 
								timing of events, ultimately enhancing their ability to perform complex, real-world tasks.
							<p>I also enjoy reading works related to Geospatial data, large language models and how to make models more reliable.
								Previously, I have completed my Master in Computer Science (by research) at <a
									href="https://www.iiit.ac.in/" target="_blank">IIIT - Hyderabad</a>,
								under the supervision of <a href="https://faculty.iiit.ac.in/~jawahar/"
									target="_blank">Prof. C.V. Jawahar</a> and Facebook mentors
								<a href="https://research.fb.com/people/pang-guan/" target="_blank">Dr. Guan Pang</a>,
								<a href="https://csc.lsu.edu/~saikat/" target="_blank">Dr. Saikat Basu</a>.
								During Masters, I was part of <a href="https://cvit.iiit.ac.in/">Center of Visual
									Information Technology Lab (CVIT)</a> and developed models to detect roads under occlusion in Satellite Imagery.</p>
								<!-- I am a part of <a href="https://cvit.iiit.ac.in/" >Center of Visual Information Technology Lab (CVIT)</a>, advised by <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank">Prof. C.V. Jawahar</a>. My research interests are <i>Computer Vision</i> and <i>Deep Learning</i>.  -->
								<!-- I have submitted my <a href="papers/MSThesis-compressed.pdf">thesis</a> in December 2018 on <i>Road Topology Extraction from Satellite images by Knowledge Sharing</i>.</p> -->

							<p>I worked as <strong>Research Engineer</strong> at <i>Facebook</i> in Spatial Computing
								Team. I was designing, training, and evaluating extraction of connected road network
								with limited set of labels and large scale noisy labels. </p>
							<!-- <p>Earlier, I did my Bachelor's in Electronics and Communications from <a
									href="https://www.rimt.ac.in/" target="_blank">RIMT</a> affiliated to <a
									href="https://ptu.ac.in/" target="_blank">Punjab Technical University</a>,
								Jalandhar.</p> -->

							<p>
								I'm always happy to meet new people. If you'd like to chat about anything/ collaborate, please reach out!
							</p>
							

							<p align=center>
								<a href="mailto:anilbatra2185@gmail.com" style="padding:5 20 5 25"><img
										src="images/email.png" alt="anilbatra2185@gmail.com"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Email</span></a> | 
								<a href="AnilBatra_CV.pdf" target="_blank" style="padding:5 20 5 25"><img
										src="images/cv.png" style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">CV</span></a> | 
								<a href="https://scholar.google.co.in/citations?hl=en&user=C9rsD2UAAAAJ" target="_blank"
									style="padding:5 20 5 25"><img src="images/scholar.jpg"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Google Scholar</span></a> | 
								<a href="https://github.com/anilbatra2185" target="_blank" style="padding:5 20 5 25"><img
										src="images/github.png" style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">Github</span></a> | 
								<a href="https://www.linkedin.com/in/anil-batra/" target="_blank"
									style="padding:5 5 5 25"><img src="images/linkedin.png"
										style="height: 25px;width: 25px;"><span style="vertical-align: middle; margin: 5px;">LinkedIn</span></a>
								<!-- <a href="https://twitter.com/_anilbatra?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false" style="padding:5 10 5 25">
									Follow @_anilbatra</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->
							</p>
						</td>
						<td width="20%" valign="top">
							<table>
								<tr>
									<td><a href="images/anilbatra.jpg"><img src="images/anilbatra.jpg" width="100%"></a></td>
								</tr>
								<tr>
									<td style="text-align: center;"><a href="https://twitter.com/_anilbatra?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @_anilbatra</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></td>
								</tr>
							</table>
						</td>
					</tr>
				</table>	
				
				<!-- <p align="center" style="color:#001affbc">
					<img src="images/blue_arrow_right.png" width="50px" height="30px" style="border-style: none"> 
					<strong style="font-size:larger;">Looking for Full-time Research Positions.</blink>
					<img src="images/blue_arrow_left.png" width="50px" height="30px" style="border-style: none"> 
				</p> -->

				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td>
							<sectionheading>News</sectionheading>
							<ul>
								<li> <b>[May 2025]</b>: Our new work "Predicting Implicit Arguments in Procedural Video Instructions" is accepted at
									<a href="https://2025.aclweb.org/" target="_blank">ACL 2025 (Main)</a>.
								</li>
								<li> <b>[July 2024]</b>: Our new work "Efficient Pre-training for Localized Instruction Generation of Videos" is accepted at
									<a href="https://docs.google.com/spreadsheets/d/1G8FQNlitoRr1oK2-LZEloeg0_VBP-E0J_WoSXqAhxNo/pubhtml" target="_blank">ECCV 2024</a>.
								</li>
								<li> <b>[Sep 2022]</b>: Our new work "Temporal Ordering in the Segmentation of
									Instructional Videos" is accepted at
									<a href="https://bmvc2022.org/programme/papers/" target="_blank">BMVC 2022</a>.
								</li>
								<li> <b>[Dec 2021]</b>: Volunteer for session chair at <a
										href="https://neurips.cc/virtual/2021/affinity-workshop/22884"
										target="_blank">LXAI</a> workshop,
									<a href="https://neurips.cc/" target="_blank">Neurips 2021</a>.
								</li>
								<li> <b>[Nov 2021]</b>: Will be serving as CVPR 2022 Reviewer.</li>
								<li> <b>[Jun 2021]</b>: Served as ICCV 2021 Reviewer.</li>
								<li> <b>[Sep 2020]</b>: Joined <a
										href="https://web.inf.ed.ac.uk/cdt/natural-language-processing"
										target="_blank">CDT-NLP</a> Ph.D at University of Edinburgh under the
									supervision of <a href="https://laurasevilla.me/" target="_blank">Dr. Laura
										Sevillia</a> and <a href="http://homepages.inf.ed.ac.uk/keller/"
										target="_blank">Prof. Frank Keller</a>.</li>
								<li> <b>[Jun 2019]</b>: Succesfully defended my Master thesis. Panel - <a
										href="https://faculty.iiit.ac.in/~jawahar/" target="_blank">Prof. C.V.
										Jawahar</a>, <a href="https://www.iiit.ac.in/people/faculty/mkrishna/"
										target="_blank">Prof. K. Madhava Krishna</a>, <a href="https://girishvarma.in/"
										target="_blank">Dr. Girish Varma</a></li>
								<li> <b>[Jun 2019]</b>: Poster presentation at CVPR 2019, Long Beach (<a
										href="images/cvpr_poster_present.jpg" target="_blank">image</a>).</li>
								<li> <b>[May 2019]</b>: Received travel sponsporship from Facebook - Spatial Computing
									team to attend CVPR 2019.</li>
								<li> <b>[Apr 2019]</b>: Presented <i>CVPR - Improving Road Connectivity</i> work at
									Facebook Spatial Team.</li>
								<li> <b>[Mar 2019]</b>: Paper accepted at CVPR 2019 on Improved Road Connectivity.</li>
								<li> <b>[Jan 2019]</b>: Join <i>Facebook - Spatial Team</i> as Research Engineer
									(Contingent Worker).</li>
								<li> <b>[Dec 2018]</b>: Submitted my Master <a
										href="papers/MSThesis-compressed.pdf">thesis</a> <i>Road Topology Extraction
										from Satellite images by Knowledge Sharing</i>.</li>
								<li> <b>[Jun 2018]</b>: Paper accepted at BMVC 2018 on <i>Self-Supervised Learning</i>.
								</li>
							</ul>
						</td>
					</tr>
				</table>

				<table width="70%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td>
							<sectionheading>Publications</sectionheading>
						</td>
					</tr>
				</table>

				<table width="70%" align="center">
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/implicit_vid_srl.png">
								<img src="images/implicit_vid_srl.png" alt="sym" width="80%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="./projects/implicit-vid-srl/index.html" id="im2025">
									<img src="images/new.png" alt="[NEW]" width="50px" height="30px" style="border-style: none">
									<heading>Predicting Implicit Arguments in Procedural Video Instructions</heading>
								</a><br>
								<strong>Anil Batra</strong>, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller<br>
								<em>ACL Main (2025)</em><br>
							</p>
							<div class="paper" id="im2025">
								<a href="./projects/implicit-vid-srl/index.html" target="_blank" class="buttonTT">Project</a>
								<!-- <a href="#" target="" class="buttonTT">suppl</a> -->
								<a data-toggle="collapse" data-parent="#im2025" href="#im2025_abstract-list"
									class="buttonAA">abstract</a>
								<!-- <a shape="rect" data-parent="#im2025" data-toggle="collapse" href="#im2025-list"
									class="buttonSS">bibtex</a> -->
								<br> 

								<div id="im2025_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
										<p style="font-size:14.1px; text-align: justify;">
											Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like 
											{<span style="color: #F28C28;">verb</span>, <span style="color: green;">what</span>, <span style="color: purple;">where/with</span>}. 
											Procedural instructions are highly elliptic, for instance, (i) <em>add cucumber to the bowl</em> and (ii) <em>add sliced tomatoes</em>, the second step's <span style="color: purple;">where</span> argument is inferred from the context, 
											referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce <em>Implicit-VidSRL</em>, a dataset that
											necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity 
											tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of <span style="color: green;">what</span> and <span style="color: purple;">where/with</span> from multimodal procedural data given the <span style="color: #F28C28;">verb</span>. 
											Lastly, we propose <em>iSRL-Qwen2-VL</em>, which achieves a 17% relative improvement in F1-score for <span style="color: green;">what-implicit</span> and a 14.7% for <span style="color: purple;">where/with-implicit</span> semantic roles
											over GPT-4o.
										</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="im2025-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
											@inproceedings{batra2025implicit,<br/>
												author    = {Batra, Anil and Sevilla-Lara, Laura and Rohrbach, Marcus and Keller, Frank},<br/>
												title     = {Predicting Implicit Arguments in Procedural Video Instructions},<br/>
												journal   = {ACL (Main)},<br/>
												year      = {2025},<br/>
											  }<br/>
									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/sieve_n_swap.png">
								<img src="images/sieve_n_swap.png" alt="sym" width="80%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="#" id="cv2024">
									<!-- <img src="images/new.png" alt="[NEW]" width="50px" height="30px" style="border-style: none"> -->
									<heading>Efficient Pre-training for Localized Instruction Generation of Videos</heading>
								</a><br>
								<strong>Anil Batra</strong>, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller<br>
								<em>European Conference on Computer Vision </em>(ECCV) 2024<br>
							</p>
							<div class="paper" id="cv2024">
								<a href="https://arxiv.org/pdf/2311.15964.pdf" target="_blank" class="buttonTT">pdf</a>
								<!-- <a href="#" target="" class="buttonTT">suppl</a> -->
								<a data-toggle="collapse" data-parent="#cv2024" href="#cv2024_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#cv2024" data-toggle="collapse" href="#cv2024-list"
									class="buttonSS">bibtex</a>
								<br> 

								<div id="cv2024_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Procedural videos show step-by-step demonstrations of tasks like recipe preparation. 
										Understanding such videos is challenging, involving the precise localization of steps and the 
										generation of textual instructions. Manually annotating steps and writing instructions is 
										costly, which limits the size of current datasets and hinders effective learning. Leveraging 
										large but noisy video-transcript datasets for pre-training can boost performance, but demands 
										significant computational resources. Furthermore, transcripts contain irrelevant content and 
										exhibit style variation compared to instructions written by human annotators. To mitigate both 
										issues, we propose a technique, Sieve-\&-Swap, to automatically curate a smaller dataset: 
										(i)~Sieve filters irrelevant transcripts and (ii)~Swap enhances the quality of the text instruction
										 by automatically replacing the transcripts with human-written instructions from a text-only recipe 
										 dataset. The curated dataset, three orders of magnitude smaller than current web-scale datasets, 
										 enables efficient training of large-scale models with competitive performance. We complement our 
										 Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step localization and 
										 instruction generation for procedural videos. When this model is pre-trained on our curated dataset, 
										 it achieves state-of-the-art performance in zero-shot and finetuning settings on YouCook2 and 
										 Tasty, while using a fraction of the computational resources.
										</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="cv2024-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@inproceedings{batra2024efficient,<br>
											title={Efficient Pre-training for Localized Instruction Generation of Procedural Videos},<br>
											author={Batra, Anil and Moltisanti, Davide and Sevilla-Lara, Laura and Rohrbach, Marcus and Keller, Frank},<br>
											booktitle={European Conference on Computer Vision},<br>
											pages={347--363},<br>
											year={2024},<br>
											organization={Springer}<br>
										  }

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/BMVC2022_thumbnail.png">
								<img src="images/BMVC2022_thumbnail.png" alt="sym" width="100%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="http://arxiv.org/abs/2209.15501" id="bmvc22">
									<!-- <img src="images/new.png" alt="[NEW]" width="50px" height="30px" style="border-style: none"> -->
									<heading>A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos</heading>
								</a><br>
								<strong>Anil Batra</strong>, Shreyank N Gowda, Frank Keller, Laura Sevilla-Lara<br>
								<em>British Machine Vision Conference</em> (BMVC), 2022 <br>
							</p>
							<div class="paper" id="bmvc22">
								<a href="https://arxiv.org/pdf/2209.15501.pdf" target="_blank" class="buttonTT">pdf</a>
								<a href="papers/BMVC2022_SODA-D_Suppl.pdf" target="_blank" class="buttonTT">suppl</a>
								<!-- <a href="papers/cvpr2019_poster.pdf" target="_blank" class="buttonTT">poster</a> -->
								<a data-toggle="collapse" data-parent="#bmvc22" href="#bmvc22_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#bmvc22" data-toggle="collapse" href="#bmvc22-list"
									class="buttonSS">bibtex</a>
								<!-- <a href="https://github.com/anilbatra2185/road_connectivity" target="_blank"
									class="buttonTT">code</a> -->
								<br>

								<div id="bmvc22_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Understanding the steps required to perform a task is an important skill for AI systems. 
										Learning these steps from instructional videos involves two subproblems: (i) identifying 
										the temporal boundary of sequentially occurring segments and (ii) summarizing
										these steps in natural language. We refer to this task as Procedure Segmentation and
										Summarization (PSS). In this paper, we take a closer look at PSS and propose three
										fundamental improvements over current methods. The segmentation task is critical, as
										generating a correct summary requires each step of the procedure to be correctly identified. 
										However, current segmentation metrics often overestimate the segmentation quality
										because they do not consider the temporal order of segments. In our first contribution,
										we propose a new segmentation metric that takes into account the order of segments,
										giving a more reliable measure of the accuracy of a given predicted segmentation. Current 
										PSS methods are typically trained by proposing segments, matching them with the
										ground truth and computing a loss. However, much like segmentation metrics, existing
										matching algorithms do not consider the temporal order of the mapping between candidate 
										segments and the ground truth. In our second contribution, we propose a matching
										algorithm that constrains the temporal order of segment mapping, and is also differentiable. 
										Lastly, we introduce multi-modal feature training for PSS, which further improves
										segmentation. We evaluate our approach on two instructional video datasets (YouCook2
										and Tasty) and observe an improvement over the state-of-the-art of ∼ 7% and ∼ 2.5%
										for procedure segmentation and summarization, respectively.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="bmvc22-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@InProceedings{batraBMVC2022pss,<br>
										author = {Batra, Anil and Gowda, Shreyank N and Keller, Frank and Sevilla-Lara, Laura},<br>
										title = {A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos},<br>
										booktitle = {BMVC},<br>
										year = {2022} <br>
										}

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" valign="top" align="center"><a href="images/cvpr_poster_method.png">
								<img src="images/cvpr_poster_method-min.png" alt="sym" width="100%"
									style="border-style: none"></a></td>
						<td width="75%" valign="top">
							<p><a href="https://github.com/anilbatra2185/road_connectivity/" id="cvpr19">
									<!--<img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">-->
									<heading>Improved Road Connectivity by Joint Learning of Orientation and
										Segmentation</heading>
								</a><br>
								<strong>Anil Batra*</strong>, Suriya Singh*, Guan Pang, Saikat Basu, C.V. Jawahar and
								Manohar Paluri (* equal contribution)<br>
								<em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019 <br>
							</p>
							<div class="paper" id="cvpr19">
								<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Batra_Improved_Road_Connectivity_by_Joint_Learning_of_Orientation_and_Segmentation_CVPR_2019_paper.pdf"
									target="_blank" class="buttonTT">pdf</a>
								<a href="papers/RoadConnectivity_CVPR_Supplementary.pdf" target="_blank"
									class="buttonTT">suppl</a>
								<a href="papers/cvpr2019_poster.pdf" target="_blank" class="buttonTT">poster</a>
								<a data-toggle="collapse" data-parent="#cvpr19" href="#cvpr19_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#cvpr19" data-toggle="collapse" href="#cvpr19-list"
									class="buttonSS">bibtex</a>
								<a href="https://github.com/anilbatra2185/road_connectivity" target="_blank"
									class="buttonTT">code</a>
								<br>

								<div id="cvpr19_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Road network extraction from satellite images often produce fragmented road
										segments leading to road maps unfit for real applications. Pixel-wise
										classification fails to predict topologically correct and connected road masks
										due to the absence of connectivity supervision and difficulty in enforcing
										topological constraints. In this paper, we propose a connectivity task called
										Orientation Learning, motivated by the human behavior of annotating roads by
										tracing it at a specific orientation. We also develop a stacked multi-branch
										convolutional module to effectively utilize the mutual information between
										orientation learning and segmentation tasks. These contributions ensure that the
										model predicts topologically correct and connected road masks. We also propose
										Connectivity Refinement approach to further enhance the estimated road networks.
										The refinement model is pre-trained to connect and refine the corrupted
										ground-truth masks and later fine-tuned to enhance the predicted road masks. We
										demonstrate the advantages of our approach on two diverse road extraction
										datasets SpaceNet and DeepGlobe. Our approach improves over the state-of-the-art
										techniques by 9% and 7.5% in road topology metric on SpaceNet and DeepGlobe,
										respectively.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>
								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="cvpr19-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@InProceedings{Batra_2019_CVPR,<br>
										author = {Batra, Anil and Singh, Suriya and Pang, Guan and Basu, Saikat and
										Jawahar, C.V. and Paluri, Manohar},<br>
										title = {Improved Road Connectivity by Joint Learning of Orientation and
										Segmentation},<br>
										booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition
										(CVPR)},<br>
										month = {June},<br>
										year = {2019} <br>
										}

									</p>
								</div>
								</pre>
							</div>
						</td>
					</tr>
					<tr>
						<td width="25%" height="80%" valign="top" align="center"><a
								href="images/overview_bmvc_logo.png">
								<img src="images/overview_bmvc_logo.png" alt="sym" width="100%"
									style="border-style: none"></a>
						</td>
						<td width="75%" valign="top">
							<p>
								<a href="http://bmvc2018.org/contents/papers/0345.pdf" id="ICLR19">
									<heading>Self-supervised Feature Learning for Semantic Segmentation of Overhead
										Imagery</heading>
								</a><br>
								Suriya Singh*, <strong>Anil Batra*</strong>, Guan Pang, Lorenzo Torresani, Saikat Basu,
								C.V. Jawahar and Manohar Paluri (* equal contribution)<br>
								<em>British Machine Vision Confernece (BMVC)</em>, 2018<br>
							</p>

							<div class="paper" id="bmvc18">
								<a href="http://bmvc2018.org/contents/papers/0345.pdf" target="_blank"
									class="buttonTT">pdf</a>
								<a href="papers/Self_supervised_Feature_Learning_for_Semantic_Segmentation_of_Overhead_Imagery_Supplementary.pdf"
									target="_blank" class="buttonTT">suppl</a>
								<a data-toggle="collapse" data-parent="#bmvc18" href="#bmvc18_abstract-list"
									class="buttonAA">abstract</a>
								<a shape="rect" data-parent="#bmvc18" href="#bmvc18-list" class="buttonSS"
									data-toggle="collapse">bibtex</a>

								<div id="bmvc18_abstract-list" class="panel-collapse collapse out"
									style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
									<p style="margin:0px 0px 0px 0px;"></p>
									<p style="font-size:14.1px; text-align: justify;">
										Overhead imageries play a crucial role in many applications such as urban
										planning, crop yield forecasting, mapping, and policy making. Semantic
										segmentation could enable automatic, efficient, and large-scale understanding of
										overhead imageries for these applications. However, semantic segmentation of
										overhead imageries is a challenging task, primarily due to the large domain gap
										from existing research in ground imageries, unavailability of large-scale
										dataset with pixel-level annotations, and inherent complexity in the task.
										Readily available vast amount of unlabeled overhead imageries share more common
										structures and patterns compared to the ground imageries, therefore, its
										large-scale analysis could benefit from unsupervised feature learning
										techniques. <br /> In this work, we study various self-supervised feature
										learning techniques for semantic segmentation of overhead imageries. We choose
										image semantic inpainting as a self-supervised task for our experiments due to
										its proximity to the semantic segmentation task. We (i) show that existing
										approaches are inefficient for semantic segmentation, (ii) propose architectural
										changes towards self-supervised learning for semantic segmentation, (iii)
										propose an adversarial training scheme for self-supervised learning by
										increasing the pretext task's difficulty gradually and show that it leads to
										learning better features, and (iv) propose a unified approach for overhead scene
										parsing, road network extraction, and land cover estimation. Our approach
										improves over training from scratch by more than 10% and ImageNet pre-trained
										network by more than 5% mIoU.</p>
									<p style="margin:0px 0px 0px 0px;"></p>
								</div>

								<p style="margin:1px 0px 0px 0px;"></p>
								<div id="bmvc18-list" class="panel-collapse collapse out"
									style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
									<p style="font-size:15px;">
										@inproceedings{singhBMVC18overhead, <br>
										Author = {Singh, Suriya; Batra, Anil; Pang, Guan; Torresani, Lorenzo; Basu,
										Saikat; Paluri, Manohar; Jawahar, C. V.}, <br>
										Title = {Self-supervised Feature Learning for Semantic Segmentation of Overhead
										Imagery}, <br>
										Booktitle = {BMVC},
										Year = {2018} <br>
										}

									</p>
								</div>
							</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>

	</table>


	<table width="70%" align="center" border="0" cellpadding="20">
		<tr>
			<td>
				<sectionheading>Professional Activities</sectionheading>
				<ul>
					<li> Reviewer: ICCV/ECCV/CVPR/ICLR/AAAI
						<!-- <a href="http://iccv2021.thecvf.com/home" target="_blank">ICCV 2021</a>,
						<a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>,
						<a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> -->
					</li>
				</ul>
			</td>
		</tr>
		<tr>
			<td>
				<sectionheading>Selected Awards</sectionheading>
				<ul>
					<li> Awarded funding for 4 years by <a href="https://www.ed.ac.uk/informatics"
							target="_blank">School of Informatics</a> and <a
							href="https://gtr.ukri.org/person/B48C147D-9AB7-4E89-8144-20FAD3E0048D"
							target="_blank">UKRI</a> for Ph.D.</li>
					<li> Facebook Travel Support to attend CVPR 2019</li>
					<li><a href="https://en.wikipedia.org/wiki/Graduate_Aptitude_Test_in_Engineering">GATE</a> - EC
						qualified with 254 rank in 2009.</li>
					<li> Gold Medal in Electronics and Communication at RIMT-Institute of Engineering and Technology,
						affiliated to PTU-Jalandhar (2007)</li>
				</ul>
			</td>
		</tr>
	</table>

	<!-- <table width="30%" align="right" border="0" cellspacing="0" cellpadding="20">
		<tr>
			<td>
				<p align="right">
					<font size="2">
						Template: <a href="https://people.eecs.berkeley.edu/~pathak/">this</a>, <a
							href="http://www.cs.berkeley.edu/~sgupta/">this</a> and <a
							href="http://jeffdonahue.com/">this</a>
					</font>
				</p>
			</td>
			<td>
				<p align="right">
					<font size="2"></font>
					<a href="https://hits.seeyoufarm.com"><img
							src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=http%3A%2F%2Fanilbatra2185.github.io&count_bg=%23A4C78A&title_bg=%23929191&icon=apollographql.svg&icon_color=%23C3E508&title=Visitors&edge_flat=false" /></a>
				</p>
			</td>
		</tr>
	</table> -->

	</td>
	</tr>
	</table>
	<script xml:space="preserve" language="JavaScript">
		hideallbibs();
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('bmvc22_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('cvpr19_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('bmvc18_abstract');
	</script>
	<script xml:space="preserve" language="JavaScript">
		hideblock('cdt20_abstract');
	</script>
</body>

</html>